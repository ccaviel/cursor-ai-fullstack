# LLM Configuration
MODEL_TYPE=huggingface  # Options: huggingface, gguf
MODEL_NAME=mistralai/Mistral-7B-Instruct-v0.2
EMBEDDING_MODEL=sentence-transformers/all-mpnet-base-v2
MODEL_TEMPERATURE=0.7
MODEL_MAX_TOKENS=2000
MODEL_QUANTIZATION=4  # 4-bit quantization for lower memory usage
USE_CPU_ONLY=false    # Set to true to force CPU usage with optimized models

# Fallback Models (automatically used in CPU mode)
CPU_MODEL=TinyLlama/TinyLlama-1.1B-Chat-v1.0
CPU_EMBEDDING_MODEL=sentence-transformers/all-MiniLM-L6-v2

# Hardware Configuration
CUDA_VISIBLE_DEVICES=0  # Set to -1 to disable GPU
NUM_THREADS=4          # Number of CPU threads for inference

# Redis Configuration
REDIS_HOST=redis
REDIS_PORT=6379
REDIS_DB=0

# Backend Configuration
HOST=0.0.0.0
PORT=8081
FLASK_ENV=development

# n8n Configuration
N8N_HOST=n8n
N8N_PORT=5678
N8N_PROTOCOL=http
N8N_AUTH_TOKEN=your_n8n_auth_token_here

# AI Agent Configuration
AGENT_TEMPERATURE=0.7
AGENT_MAX_STEPS=10
AGENT_MEMORY_SIZE=5
AGENT_CACHE_TTL=3600

# Vector Store Configuration
VECTOR_STORE_TYPE=chroma
CHUNK_SIZE=1000
CHUNK_OVERLAP=200

# Optional: Debug mode
DEBUG=False 